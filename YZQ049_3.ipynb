{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmyRd+NfSFuVhltAE0pV8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdrianRamos956/Homeworks/blob/main/YZQ049_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-3WWyXD7nl2"
      },
      "source": [
        "Lab 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Oq_JoLT9uXoT",
        "outputId": "90a658c2-0034-48cf-f8de-3b07b5ec3153"
      },
      "source": [
        "#This begining of the code is imporing everyhting we will need for this lab\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import svm\n",
        "#This gets the data.csv and reads it and stores it all into df\n",
        "df=pd.read_csv(\"data.csv\")\n",
        "#This makes the Diagnosis form M for Malignant and B for Benign Binary so we can actually use them in these Classifications\n",
        "df['diagnosis']=df['diagnosis'].map({'M':1,'B':0})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302          1  ...                  0.11890          NaN\n",
              "1    842517          1  ...                  0.08902          NaN\n",
              "2  84300903          1  ...                  0.08758          NaN\n",
              "3  84348301          1  ...                  0.17300          NaN\n",
              "4  84358402          1  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BycIF9RlvUor"
      },
      "source": [
        "#Sets X to be everything important (We exclude ID because that is not used in diagnosing someone at all)\n",
        "X = df[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean','fractal_dimension_mean',\n",
        "                 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n",
        "                 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst','fractal_dimension_worst']]\n",
        "\n",
        "#OF course out Y will be the diagnosis isself it is the yes or the no\n",
        "y = df['diagnosis']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ej7vQX-Oa26"
      },
      "source": [
        "I chose to use Linear Regression, SVM, and KNN because i though that these would be the best to use since they are all so versitile in what they do and how you can use them for almost any situation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBGA_bi8ghq"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1_NVK7hbDHK",
        "outputId": "3da292e6-5570-4354-b551-29d288aa2c7f"
      },
      "source": [
        "#This is the spliter to train it nice and well\n",
        "X_trainLR, X_testLR, y_trainLR, y_testLR = train_test_split(X,y,test_size=0.3,random_state=109)\n",
        "\n",
        "#We want to scale the data so it is nice\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_trainLR = scaler.fit_transform(X_trainLR)\n",
        "X_testLR = scaler.transform(X_testLR)\n",
        "\n",
        "#Initialize Logistic regression\n",
        "LR = LogisticRegression()\n",
        "\n",
        "#We fit it\n",
        "LR.fit(X_trainLR, y_trainLR)\n",
        "\n",
        "#We predict it\n",
        "y_predLR = LR.predict(X_testLR)\n",
        "\n",
        "#This is the accuracy output\n",
        "accuracy = metrics.accuracy_score(y_testLR, y_predLR)\n",
        "accuracy_percentage = 100 * accuracy\n",
        "accuracy_percentage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.24561403508771"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6nR2BlO7j_3",
        "outputId": "1336e307-c5be-40cb-b068-ae52a20f4466"
      },
      "source": [
        "#This is the Confusion Matrix the professor asked for\n",
        "print(confusion_matrix(y_testLR, y_predLR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[108   0]\n",
            " [  3  60]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMySKD0V6kzu",
        "outputId": "f2fe8cde-31da-43e0-edf3-86d901f8dadb"
      },
      "source": [
        "#This is the Precision, Recall, F-measures the professor asked for\n",
        "print(classification_report(y_testLR, y_predLR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       108\n",
            "           1       1.00      0.95      0.98        63\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.99      0.98      0.98       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P01WBRoQ8l-R"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaPmSc9MbOsj",
        "outputId": "d2560c44-253b-4646-b133-c252bffd26cd"
      },
      "source": [
        "#This is the spliter to train it nice and well\n",
        "X_trainSV, X_testSV, y_trainSV, y_testSV = train_test_split(X,y,test_size=0.3,random_state=109)\n",
        "\n",
        "#We want to scale the data so it is nice\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_trainSV = scaler.fit_transform(X_trainSV)\n",
        "X_testSV = scaler.transform(X_testSV)\n",
        "\n",
        "#Initialize SVM\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "#We fit it\n",
        "clf.fit(X_trainSV, y_trainSV)\n",
        "\n",
        "#We predict it\n",
        "y_predSV = clf.predict(X_testSV)\n",
        "\n",
        "#This is the accuracy output\n",
        "accuracy = metrics.accuracy_score(y_testSV, y_predSV)\n",
        "accuracy_percentage = 100 * accuracy\n",
        "accuracy_percentage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.83040935672514"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoYguTfz7h-5",
        "outputId": "6039a7ad-3825-41fb-f377-6ca256096de8"
      },
      "source": [
        "#This is the Confusion Matrix the professor asked for\n",
        "print(confusion_matrix(y_testSV, y_predSV))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[108   0]\n",
            " [  2  61]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfIRjVTg6oI0",
        "outputId": "c28c1082-3bd8-4cb3-bb2c-d2d60ed695ab"
      },
      "source": [
        "#This is the Precision, Recall, F-measures the professor asked for\n",
        "print(classification_report(y_testSV, y_predSV))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       108\n",
            "           1       1.00      0.97      0.98        63\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.99      0.98      0.99       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUozAAA8pFj"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSSDkTxs3EJc",
        "outputId": "1180a9b4-e49f-4ea5-9bdc-1ae245058197"
      },
      "source": [
        "#This is the spliter to train it nice and well\n",
        "X_trainKN, X_testKN, y_trainKN, y_testKN = train_test_split(X,y,test_size=0.3,random_state=109)\n",
        "\n",
        "#We want to scale the data so it is nice\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_trainKN = scaler.fit_transform(X_trainKN)\n",
        "X_testKN = scaler.transform(X_testKN)\n",
        "\n",
        "#Initialize KNN with the default neighbors\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "#We fit it\n",
        "classifier.fit(X_trainKN, y_trainKN)\n",
        "\n",
        "#We predict it\n",
        "y_predKN = classifier.predict(X_testKN)\n",
        "\n",
        "#This is the accuracy output\n",
        "accuracy = metrics.accuracy_score(y_testKN, y_predKN)\n",
        "accuracy_percentage = 100 * accuracy\n",
        "accuracy_percentage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.49122807017544"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMOrvad7f5n",
        "outputId": "e3ef2dbc-a237-436b-d848-a08cf82470f4"
      },
      "source": [
        "#This is the Confusion Matrix the professor asked for\n",
        "print(confusion_matrix(y_testKN, y_predKN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[106   2]\n",
            " [  4  59]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T6DNP6N6qWG",
        "outputId": "f7ad0fd9-6788-48a1-9c6a-bad68e9b750a"
      },
      "source": [
        "#This is the Precision, Recall, F-measures the professor asked for\n",
        "print(classification_report(y_testKN, y_predKN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       108\n",
            "           1       0.97      0.94      0.95        63\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.96      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWFsN8bHOtpu"
      },
      "source": [
        "In the end it seems that with the given data set and with everything that i have set up everything it seems as though SVM is the best one for the Breast Cancer statistics becasue it had a Accuracy of 98.83"
      ]
    }
  ]
}